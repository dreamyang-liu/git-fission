/**
 * LLM-related functions for git-fission
 */

import { BedrockRuntimeClient, ConverseCommand } from '@aws-sdk/client-bedrock-runtime';
import fetch from 'node-fetch';
import type { CommitInfo, LLMAnalysis, LLMConfig, LLMProvider, SplitPlan } from './types.js';

/**
 * Call AWS Bedrock
 */
async function callBedrock(prompt: string, model: string, maxTokens: number): Promise<string | null> {
  const bearerToken = process.env.AWS_BEARER_TOKEN_BEDROCK;
  const region = process.env.AWS_REGION || 'us-west-2';

  if (bearerToken) {
    const endpoint = `https://bedrock-runtime.${region}.amazonaws.com/model/${model}/converse`;
    const body = JSON.stringify({
      messages: [{ role: 'user', content: [{ text: prompt }] }],
      inferenceConfig: { maxTokens, temperature: 0.1 },
      anthropic_beta: ['context-1m-2025-08-07']
    });

    try {
      const resp = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${bearerToken}` },
        body,
      });
      const data = await resp.json() as any;
      return data.output?.message?.content?.[0]?.text || null;
    } catch (e) {
      return null;
    }
  } else {
    const client = new BedrockRuntimeClient({ region });
    try {
      const resp = await client.send(new ConverseCommand({
        modelId: model,
        messages: [{ role: 'user', content: [{ text: prompt }] }],
        inferenceConfig: { maxTokens, temperature: 0.1 },
      }));
      return (resp.output?.message?.content?.[0] as any)?.text || null;
    } catch (e) {
      return null;
    }
  }
}

/**
 * Call Anthropic API directly
 */
async function callAnthropic(prompt: string, model: string, maxTokens: number, apiKey?: string): Promise<string | null> {
  const key = apiKey || process.env.ANTHROPIC_API_KEY;
  if (!key) {
    console.error('ANTHROPIC_API_KEY not set');
    return null;
  }

  try {
    const resp = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': key,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model,
        max_tokens: maxTokens,
        temperature: 0.1,
        messages: [{ role: 'user', content: prompt }],
      }),
    });
    const data = await resp.json() as any;
    if (data.error) {
      console.error('Anthropic API error:', data.error.message);
      return null;
    }
    return data.content?.[0]?.text || null;
  } catch (e) {
    console.error('Anthropic API call failed:', e);
    return null;
  }
}

/**
 * Call OpenAI API
 */
async function callOpenAI(prompt: string, model: string, maxTokens: number, apiKey?: string): Promise<string | null> {
  const key = apiKey || process.env.OPENAI_API_KEY;
  if (!key) {
    console.error('OPENAI_API_KEY not set');
    return null;
  }

  try {
    const resp = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${key}`,
      },
      body: JSON.stringify({
        model,
        max_completion_tokens: maxTokens,
        temperature: 0.1,
        messages: [{ role: 'user', content: prompt }],
      }),
    });
    const data = await resp.json() as any;
    if (data.error) {
      console.error('OpenAI API error:', data.error.message);
      return null;
    }
    return data.choices?.[0]?.message?.content || null;
  } catch (e) {
    console.error('OpenAI API call failed:', e);
    return null;
  }
}

/**
 * Call OpenRouter API
 */
async function callOpenRouter(prompt: string, model: string, maxTokens: number, apiKey?: string): Promise<string | null> {
  const key = apiKey || process.env.OPENROUTER_API_KEY;
  if (!key) {
    console.error('OPENROUTER_API_KEY not set');
    return null;
  }

  try {
    const resp = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${key}`,
      },
      body: JSON.stringify({
        model,
        max_tokens: maxTokens,
        temperature: 0.1,
        messages: [{ role: 'user', content: prompt }],
      }),
    });
    const data = await resp.json() as any;
    if (data.error) {
      console.error('OpenRouter API error:', data.error.message);
      return null;
    }
    return data.choices?.[0]?.message?.content || null;
  } catch (e) {
    console.error('OpenRouter API call failed:', e);
    return null;
  }
}

/**
 * Main LLM call function - routes to the appropriate provider
 */
export async function callLLM(prompt: string, config: LLMConfig, maxTokens = 1024): Promise<string | null> {
  switch (config.provider) {
    case 'bedrock':
      return callBedrock(prompt, config.model, maxTokens);
    case 'anthropic':
      return callAnthropic(prompt, config.model, maxTokens, config.apiKey);
    case 'openai':
      return callOpenAI(prompt, config.model, maxTokens, config.apiKey);
    case 'openrouter':
      return callOpenRouter(prompt, config.model, maxTokens, config.apiKey);
    default:
      console.error(`Unknown provider: ${config.provider}`);
      return null;
  }
}

/**
 * Parse model string into LLMConfig
 * Supports formats:
 *   - "provider:model" (e.g., "anthropic:claude-3-5-sonnet-20241022")
 *   - "model" (uses default provider from env or bedrock)
 */
export function parseModelString(modelStr: string, defaultProvider: LLMProvider = 'bedrock'): LLMConfig {
  const colonIndex = modelStr.indexOf(':');

  // Check if it looks like "provider:model"
  const providers: LLMProvider[] = ['bedrock', 'anthropic', 'openai', 'openrouter'];
  if (colonIndex > 0) {
    const prefix = modelStr.substring(0, colonIndex);
    if (providers.includes(prefix as LLMProvider)) {
      return {
        provider: prefix as LLMProvider,
        model: modelStr.substring(colonIndex + 1),
      };
    }
  }

  // No provider prefix - use default provider
  return {
    provider: defaultProvider,
    model: modelStr,
  };
}

export async function analyzeWithLLM(commit: CommitInfo, config: LLMConfig): Promise<LLMAnalysis | null> {
  const filesSum = commit.files.slice(0, 20).map(f => `  - ${f}`).join('\n');
  const prompt = `Analyze this git commit and determine if it is ATOMIC (does exactly one logical thing).

**Commit Message:** ${commit.message}
**Stats:** ${commit.filesChanged} files changed, +${commit.insertions}/-${commit.deletions} lines
**Files Changed:**
${filesSum}

**Diff (may be truncated):**
\`\`\`
${commit.diff || '(diff not available)'}
\`\`\`

Respond in JSON format:
{
  "is_atomic": true/false,
  "confidence": 0.0-1.0,
  "reasoning": "Brief explanation",
  "concerns": ["list of concerns if not atomic"],
  "split_suggestion": "How to split, or null if atomic"
}

Only output the JSON.`;

  const response = await callLLM(prompt, config);
  if (!response) return null;

  try {
    const match = response.match(/\{[\s\S]*\}/);
    if (!match) return null;
    const result = JSON.parse(match[0]);
    return {
      isAtomic: result.is_atomic,
      confidence: result.confidence,
      reasoning: result.reasoning,
      concerns: result.concerns || [],
      splitSuggestion: result.split_suggestion,
    };
  } catch { return null; }
}

import { parseDiffIntoHunks, rebuildPatchFromHunks, type ParsedFileDiff } from './git.js';

/**
 * Hunk classification result from LLM
 */
export interface HunkClassification {
  commits: Array<{
    message: string;
    description: string;
    hunkIds: number[];
  }>;
  reasoning: string;
}

/**
 * Format hunk content for LLM display - show actual changes
 */
function formatHunkForLLM(hunk: { id: number; startLine: number; content: string }, maxLines: number = 15): string {
  const lines = hunk.content.split('\n');
  const changedLines = lines.filter(l => l.startsWith('+') || l.startsWith('-'));

  // Show up to maxLines of actual changes
  const displayLines = changedLines.slice(0, maxLines);
  let display = displayLines.map(l => `    ${l}`).join('\n');

  if (changedLines.length > maxLines) {
    display += `\n    ... (${changedLines.length - maxLines} more lines)`;
  }

  return display;
}

/**
 * Ask LLM to classify hunks into commits
 */
export async function classifyHunks(
  commit: CommitInfo,
  files: ParsedFileDiff[],
  config: LLMConfig,
  instruction?: string
): Promise<HunkClassification | null> {
  // Build display for LLM - show hunks with their IDs and actual diff content
  let hunksDisplay = '';
  for (const file of files) {
    hunksDisplay += `\n**${file.filePath}:**\n`;
    for (const hunk of file.hunks) {
      hunksDisplay += `  [Hunk ${hunk.id}] starting at line ${hunk.startLine}:\n`;
      hunksDisplay += formatHunkForLLM(hunk) + '\n';
    }
  }

  const customInstruction = instruction ? `\n**Custom Instruction:** ${instruction}\n` : '';

  const prompt = `You are a git expert. Analyze this commit and decide how to split it into atomic commits.

**Original Commit Message:** ${commit.message}
**Files Changed:** ${commit.filesChanged}
**Stats:** +${commit.insertions}/-${commit.deletions} lines
${customInstruction}
**Hunks to classify (each hunk is a contiguous block of changes):**
${hunksDisplay}

TASK: Classify each hunk into one of 2-5 atomic commits. Each commit should do ONE logical thing.

Rules:
1. Every hunk ID must appear in exactly ONE commit
2. Related changes should stay together (e.g., a function and its callers)
3. Hunks from the same file that are related should go together
4. Import statement hunks should go with the code that uses them
5. **Dependency Order**: Order commits so dependencies are introduced BEFORE code that uses them. If commit B depends on code from commit A, commit A must come first.
6. **Build-ability**: Each commit must be independently buildable. Don't split in a way that would break compilation (e.g., adding a function call without the function definition, using a type before it's defined).
7. **No Forward References**: If new code references other new code in the same commit, they must stay together or the referenced code must come in an earlier commit.

Respond in JSON:
{
  "reasoning": "Brief explanation of how you're splitting this",
  "commits": [
    {
      "message": "feat(auth): Add login function",
      "description": "What this commit does",
      "hunkIds": [0, 2, 5]
    }
  ]
}

If the commit is already atomic, return a single commit with all hunk IDs.
Only output the JSON.`;

  const response = await callLLM(prompt, config, 8192);
  if (!response) {
    console.error('  LLM returned no response');
    return null;
  }

  try {
    const match = response.match(/\{[\s\S]*\}/);
    if (!match) {
      console.error('  No JSON found in response:', response.slice(0, 200));
      return null;
    }
    return JSON.parse(match[0]);
  } catch (e) {
    console.error('  Failed to parse JSON:', e);
    return null;
  }
}

/**
 * Generate split plan using hunk-level classification
 */
export async function generateSplitPlan(commit: CommitInfo, config: LLMConfig, instruction?: string): Promise<SplitPlan | null> {
  if (!commit.diff) return null;

  // Parse diff into hunks
  const files = parseDiffIntoHunks(commit.diff);

  const totalHunks = files.reduce((sum, f) => sum + f.hunks.length, 0);
  if (totalHunks === 0) {
    return null;
  }

  console.log(`  ${totalHunks} hunks across ${files.length} files`);

  // Ask LLM to classify hunks
  const classification = await classifyHunks(commit, files, config, instruction);
  if (!classification) return null;

  // Build patches from hunk classification
  const splits = classification.commits.map(c => ({
    message: c.message,
    description: c.description,
    diff: rebuildPatchFromHunks(files, c.hunkIds),
  }));

  return {
    reasoning: classification.reasoning,
    splits,
  };
}

